<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.0/lazysizes.min.js"></script>
		<title>Vedant Shah</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<style>#topRightElem{
			position: relative;
			margin-top: auto;
			/* margin-left: auto; */
		}</style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">		
				<div class="logo" id="topRightElem">
					<h3><span class="icon solid fa-home"><a href="index.html#"> HOME </a> </span></h3>
				</div>

				<!-- Header -->
				<header id="header">
					<h2 class="major">Projects</h2>
					<nav>
						<ul>
							<li><a href="#GBI">Gesture Based Interaction</a></li>
							<li><a href="#MGML">Multimodal Generative Machine Learning</a></li>
							<li><a href="#TCA">Text Classification and Analytics</a></li>
						</ul>
					</nav>
				</header>

				<!-- Main -->
					<div id="main">
							<!-- Projects -->
							<article id="GBI">
								<h2>
									Gesture Control
								</h2>
									<br>
									<h4><li>
										Volume Control
									</li></h4>
								<span class="image main"><img src="overlay.png" data-src="images/vid1.gif" width="450" height="300" class="lazyload" style="filter:brightness(110%)" alt="" /></span>
								<p>
								   Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and alter the sound of the host system. This version
								   is designed for desktop use.
								   <br>
								   <a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/VolumeHandControl.py"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								   &nbsp;
								   <a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4>
									<li>
										Virtual Mouse Control
								    </li>
							    </h4>
								<span class="image main"><img src="overlay.png" data-src="images/vid2.gif" width="450" height="300" class="lazyload" style="filter:brightness(110%)" alt="" /></span>
								<p>
								Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and move the mouse pointer on the desktop.
								Further, this model also supports single and double clicks with certain finger movements. This version is designed for desktop use.
								<br>
								<a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/AI_Virtual_Mouse.py"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4>
									<li>
										Virtual Keyboard
								    </li>
							    </h4>
								<span class="image main"><img src="overlay.png" data-src="images/vid3.gif" width="450" height="300" class="lazyload" style="filter:brightness(110%)" alt="" /></span>
								<p>
								Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and type the letters as selected on the desktop screen.
								This version is designed for desktop use and is able to write text on popular apps such as notepad++, Word, etc.
								<br>
								<a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/AI_KeyBoard.py"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h4>
									<li>
										Virtual Painter
								    </li>
							    </h4>
								<span class="image main"><img src="overlay.png" data-src="images/vid4.gif" width="450" height="300" class="lazyload" style="filter:brightness(110%)" alt="" /></span>
								<p>
								Brief : A gesture control mechanism which makes use of a camera to capture the finger movements of a hand and paint on the desktop screen
								within the reference box. Currently it supports four different colors and the eraser option.
								This version is designed for desktop use.
								<br>
								<a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/AI_Painter.py"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/ComputerVisionBasics/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
							</article>

							<article id="MGML">
								<h4>
									Keywords: GANs, VAEs, Classifiers, Computer Vision.
								</h4>
									<br>
									<h3><li>
										Variational AutoEncoder for Image Generation
									</li></h3>
								<figure>	
								<span class="image main"><img src="overlay.png" data-src="images/pic06.jpg" width="400" height="450" class="lazyload" style="filter:brightness(100%)" alt="" /></span>
								<figcaption style="font-weight:200;">Img Title: Latent Space Distribution of the Variational AutoEncoder (VAE)</figcaption>
							    </figure>
								<br>
								<p>
								Brief : A Variational Autoencoder (VAE) is a machine learning model that learns to generate new data by understanding the underlying patterns in the training dataset. It uses 
								probabilistic techniques to capture the distribution of the data and produces diverse outputs. I have developed this VAE using TensorFlow and the Keras 
								functional API to perform dataset generation. The VAE model has been trained on the  <a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist"><em>fashion MNIST dataset</em></a>. 
								The depicted figure above illustrates the distribution of the latent space in the model, while the subsequent figure below showcases the output generated by my VAE.
								<br>
								<span class="image main"><img src="overlay.png" data-src="images/pic07.jpg" width="400" height="200" class="lazyload" style="filter:brightness(100%)" alt="" /></span>
								<a href="https://github.com/sVedantWork/ImageGeneration/blob/main/VAE.ipynb"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/ImageGeneration/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h3><li>
									Variational AutoEncoder for Sound Generation
								</li></h3>
								<div>
									<strong>Original Sound</strong><br>
									<audio controls>
										<source src="sounds/og01.mp3" type="audio/mpeg">
									</audio>
									<br>
									<strong>Generated Sound</strong><br>
									<audio controls>
										<source src="sounds/gen01.mp3" type="audio/mpeg">
									</audio>
								</div><br>
								<p>
								Brief :  A Variational Autoencoder (VAE) is a machine learning model that learns to generate new data by understanding the underlying patterns in the training dataset. 
								It uses probabilistic techniques to capture the distribution of the data and produces diverse outputs. I have implemented a Variational Autoencoder (VAE) using TensorFlow
								and the Keras functional API for the purpose of sound generation. This VAE model has been trained on the <a href="https://www.kaggle.com/datasets/joserzapata/free-spoken-digit-dataset-fsdd"><em>
								free spoken digits dataset</em></a>. The audio representation embedded above depicts the original audio utilized during the training process, as well as the audio generated
								by the model, which is generated based on sampled mel spectrograms.
							    <br>
								<a href="https://github.com/sVedantWork/Sound-Generation"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/Sound-Generation/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h3><li>
									Generative Adversarial Network for Image Generation 
								</li></h3>
								<figure>
							    <span class="image main"><img src="overlay.png" data-src="images/pic08.jpg" width="450" height="250" class="lazyload" style="filter:brightness(100%)" alt="" /></span>
								<figcaption style="font-weight:200;">Img Title: Results produced by the GAN</figcaption>
							    </figure><br>
								<p>
								Brief : Generative Adversarial Networks (GANs) similar to VAEs learn to generate new data and consist of two components a generator and a discriminator. The generator 
								learns to generate new data samples that resemble the training data, while the discriminator learns to distinguish between real and generated data. This adversarial 
								training process leads to the generation of high-quality and realistic data samples. I've built a GAN model from scratch using TensorFlow and the Keras Sequential API, 
								with the aim of dataset generation. The image displayed above showcases the achieved results. Throughout the project, the GAN was trained on the  
						        <a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist"><em>fashion MNIST dataset</em></a>. My ongoing projects involve further enhancing the
								quality of the generated images based on the foundation established by this project.
								<br>
								<a href="https://github.com/sVedantWork/ImageGeneration/blob/main/Generative_Adversarial_Networks.ipynb"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/ImageGeneration/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
								<h3><li>
									Emotion Recognition from Short-Videos 
								</li></h3>
								<figure>
							    <span class="image main"><img src="overlay.png" data-src="images/pic10.png" width="450" height="480" class="lazyload" style="filter:brightness(100%)" alt="" /></span>
								<figcaption style="font-weight:200;">Img Title: Confusion Matrix of Classifier's Performance Across different Emotions</figcaption>
							    </figure><br>
								<p> 
								Brief : Advanced machine learning techniques were applied to develop and implement an emotion classification system using PyTorch. Three deep learning models - ResNext50, ResNext101, and VGG19, 
								were trained on the <a href="https://www.kaggle.com/datasets/msambare/fer2013"><em>FER_2013_Kaggle</em></a> dataset. The models were evaluated on metrics such as Cohen's Kappa and Balanced Accuracy
								from Scikit-Learn (see image below). The ResNext101 model yielded the highest accuracy at 67% on test set. An emotion recognition pipeline was created which accepts a short video (up to 5 seconds), 
								identifies, and ranks the top 3 emotions. This module is part of a larger music recommendation system, utilizing the emotion classification to provide suitable music suggestions.
								<br>
								<span class="image main"><img src="overlay.png" data-src="images/pic11.png" width="400" height="300" class="lazyload" style="filter:brightness(100%)" alt="" /></span>
								<a href="https://github.com/sVedantWork/Video2Music"><span class="icon brands fa-github"></span> <strong> Code</strong></a>
								&nbsp;
								<a href="https://github.com/sVedantWork/Video2Music/blob/main/README.md"><span class="icon solid fa-paperclip"></span> <strong> Read Me</strong></a>
								</p>
							</article>

							<article id="TCA">					
									<h4>
										Keywords: NLP, SCIBERT, BIOBERT, GPT-2, BigBird.
									</h4>
										<br>
										<h3><li>
											Document Length Text Classification (On-Going)
										</li></h3>
									<figure>	
									<span class="image main"><img src="overlay.png" data-src="images/pic12.jpg" width="350" height="500" class="lazyload" style="filter:brightness(100%)" alt="" /></span>
									<figcaption style="font-weight:200;">Img Title: Classifying Long Text Documents into 1 of n categories (n=4 here)</figcaption>
									</figure>
									<br>
									<p>
									Brief : In this key project at Virginia Tech, I've co-developed an advanced text classification pipeline to categorize Electronic Dissertations and Theses (ETDs) into 
									200 unique STEM and Non-STEM disciplines using state-of-the-art natural language processing models—SciBERT, BioBERT, GPT-2, and BigBird. Fine-tuned on a custom dataset, the models decipher complex patterns within the documents, enhancing the web accessibility and keyword indexability of ETDs.
									</p>
									<br>
							</article>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; 2025 by Vedant Shah designed with: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>
				
					</div>
				
				<!-- BG -->
					<div id="bg"></div>
		
				<!-- Scripts -->
					<script src="assets/js/jquery.min.js"></script>
					<script src="assets/js/browser.min.js"></script>
					<script src="assets/js/breakpoints.min.js"></script>
					<script src="assets/js/util.js"></script>
					<script src="assets/js/main.js"></script>
	</body>
</html>					
